# ğŸ“ RAG-Based AI Teaching Assistant with Video Timestamps

This project is an **end-to-end Retrieval-Augmented Generation (RAG) system** built on **course videos**.  
It allows users to ask questions like **â€œWhere is Exercise 1 taught?â€** and get the **exact video name, video number, and timestamp (mm:ss)** where the topic or exercise is explained.

The system is implemented step-by-step using **Whisper, embeddings, vector similarity search, and LLMs**, and is finally exposed through an interactive **Streamlit web app**.

---

## ğŸš€ What This Project Does

- Converts course videos â†’ audio (MP3)
- Transcribes and translates audio using **Whisper**
- Splits transcripts into **timestamped chunks**
- Creates **vector embeddings** using `bge-m3` (Ollama)
- Performs **semantic similarity search**
- Uses an **LLM (llama3.2)** to return:
  - ğŸ“º Video Title  
  - ğŸ”¢ Video Number  
  - â± Exact Timestamp (mm:ss â€“ mm:ss)  
  - ğŸ“– What is practiced  
  - ğŸ‘‰ Clear guidance on where to watch  

---

## ğŸ§  Complete Workflow (Step-by-Step)

1. Video â†’ MP3 (FFmpeg)  
2. MP3 â†’ Text (Whisper transcription + translation)  
3. Text â†’ Chunks (with start & end timestamps)  
4. Chunks â†’ Embeddings (Ollama + bge-m3)  
5. Query â†’ Similarity Search (cosine similarity)  
6. LLM response with precise video & timestamp  
7. Streamlit UI for interactive querying  

---

## ğŸ›  Tech Stack

- Frontend: Streamlit  
- Speech-to-Text: OpenAI Whisper  
- Embeddings: bge-m3 (via Ollama)  
- LLM: llama3.2 (via Ollama)  
- Vector Search: scikit-learn (cosine similarity)  
- Data Handling: Pandas, Joblib  
- Language: Python  

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ step1_video_to_mp3.py          # Video â†’ Audio  
â”œâ”€â”€ step2_mp3_to_json.py           # Audio â†’ Transcription  
â”œâ”€â”€ step3_preprocess_json.py       # Chunking with timestamps  
â”œâ”€â”€ step4_code.py                  # Embedding generation  
â”œâ”€â”€ step5_process_incoming.py      # Query processing  
â”œâ”€â”€ step6_app.py                   # Streamlit application  
â”‚
â”œâ”€â”€ audios/                        # Extracted audio files  
â”œâ”€â”€ jsons/                         # Transcript & embedding JSONs  
â”œâ”€â”€ whisper/                       # Whisper outputs  
â”œâ”€â”€ embeddings.joblib              # Vector store (ignored)  
â”œâ”€â”€ prompt.txt                     # LLM prompt  
â””â”€â”€ response.txt                   # Model response  

---


## âœ¨ Key Highlights
- End-to-end RAG pipeline on real course videos
- Timestamp-level answers (mm:ss accuracy)
- Fully offline system (no paid APIs)
- Exercise & practiceâ€“focused retrieval
- Built with Whisper + Ollama + Streamlit

## ğŸ¯ Problem Statement
Long course videos make it difficult to quickly find
where a specific topic or exercise is explained.
This project solves that by enabling semantic search
and exact timestamp-based navigation.

## ğŸ§ª Example Output

Question:
Where is Exercise 1 taught?

Answer:
â€¢ Video Title: HTML Forms Tutorial  
â€¢ Video Number: 07  
â€¢ Timestamp: 12:30 â€“ 18:45  
â€¢ Guidance: Open video 07 and start watching from 12:30

## âš™ï¸ How to Update Videos
1. Add new videos to `video/`
2. Run:
   - step1_video_to_mp3.py
   - step2_mp3_to_json.py
   - step3_process_incoming.py
3. Restart Streamlit app


## ğŸ” Design Decisions
- Used cosine similarity for fast semantic matching
- Chunk-based retrieval for precise timestamps
- Local LLM via Ollama for privacy & cost efficiency
- Strict prompt to avoid hallucinations

## Architecture Flow (Step-by-Step)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Video     â”‚
â”‚  Files (.mp4)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Video â†’ Audioâ”‚
â”‚ (MP4 â†’ MP3)          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Speech-to-Text     â”‚
â”‚ (Whisper Transcription)    â”‚
â”‚ + Timestamps               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Text Preprocessing â”‚
â”‚ - Cleaning                 â”‚
â”‚ - Chunking                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Embedding Creation â”‚
â”‚ (Sentence Embeddings)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Store (FAISS)       â”‚
â”‚ - Embeddings + Metadata    â”‚
â”‚ - Timestamps               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: User Query         â”‚
â”‚ â†’ Query Embedding          â”‚
â”‚ â†’ Similarity Search        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Result Generation  â”‚
â”‚ - Best Matching Segment    â”‚
â”‚ - Exact Timestamp          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## ğŸš€ Future Improvements

- Improve timestamp accuracy using finer text chunking  
- Enhance semantic search with better embedding models  
- Add keyword + semantic hybrid search  
- Optimize performance for faster query response  
- Support multiple languages  
- Develop a user-friendly web interface  
- Enable cloud-based storage and processing  
- Add user feedback to improve result relevance  
