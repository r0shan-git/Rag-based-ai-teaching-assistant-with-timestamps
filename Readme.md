# ğŸ“ RAG-Based AI Teaching Assistant with Video Timestamps

This project is an **end-to-end Retrieval-Augmented Generation (RAG) system** built on **course videos**.  
It allows users to ask questions like **â€œWhere is Exercise 1 taught?â€** and get the **exact video name, video number, and timestamp (mm:ss)** where the topic or exercise is explained.

The system is implemented step-by-step using **Whisper, embeddings, vector similarity search, and LLMs**, and is finally exposed through an interactive **Streamlit web app**.

---

## ğŸš€ What This Project Does

- Converts course videos â†’ audio (MP3)
- Transcribes and translates audio using **Whisper**
- Splits transcripts into **timestamped chunks**
- Creates **vector embeddings** using `bge-m3` (Ollama)
- Performs **semantic similarity search**
- Uses an **LLM (llama3.2)** to return:
  - ğŸ“º Video Title  
  - ğŸ”¢ Video Number  
  - â± Exact Timestamp (mm:ss â€“ mm:ss)  
  - ğŸ“– What is practiced  
  - ğŸ‘‰ Clear guidance on where to watch  

---

## ğŸ§  Complete Workflow (Step-by-Step)

1. Video â†’ MP3 (FFmpeg)  
2. MP3 â†’ Text (Whisper transcription + translation)  
3. Text â†’ Chunks (with start & end timestamps)  
4. Chunks â†’ Embeddings (Ollama + bge-m3)  
5. Query â†’ Similarity Search (cosine similarity)  
6. LLM response with precise video & timestamp  
7. Streamlit UI for interactive querying  

---

## ğŸ›  Tech Stack

- Frontend: Streamlit  
- Speech-to-Text: OpenAI Whisper  
- Embeddings: bge-m3 (via Ollama)  
- LLM: llama3.2 (via Ollama)  
- Vector Search: scikit-learn (cosine similarity)  
- Data Handling: Pandas, Joblib  
- Language: Python  

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ step1_video_to_mp3.py          # Video â†’ Audio  
â”œâ”€â”€ step2_mp3_to_json.py           # Audio â†’ Transcription  
â”œâ”€â”€ step3_preprocess_json.py       # Chunking with timestamps  
â”œâ”€â”€ step4_code.py                  # Embedding generation  
â”œâ”€â”€ step5_process_incoming.py      # Query processing  
â”œâ”€â”€ step6_app.py                   # Streamlit application  
â”‚
â”œâ”€â”€ audios/                        # Extracted audio files  
â”œâ”€â”€ jsons/                         # Transcript & embedding JSONs  
â”œâ”€â”€ whisper/                       # Whisper outputs  
â”œâ”€â”€ embeddings.joblib              # Vector store (ignored)  
â”œâ”€â”€ prompt.txt                     # LLM prompt  
â””â”€â”€ response.txt                   # Model response  

---


## âœ¨ Key Highlights
- End-to-end RAG pipeline on real course videos
- Timestamp-level answers (mm:ss accuracy)
- Fully offline system (no paid APIs)
- Exercise & practiceâ€“focused retrieval
- Built with Whisper + Ollama + Streamlit

## ğŸ¯ Problem Statement
Long course videos make it difficult to quickly find
where a specific topic or exercise is explained.
This project solves that by enabling semantic search
and exact timestamp-based navigation.

## ğŸ§ª Example Output

Question:
Where is Exercise 1 taught?

Answer:
â€¢ Video Title: HTML Forms Tutorial  
â€¢ Video Number: 07  
â€¢ Timestamp: 12:30 â€“ 18:45  
â€¢ Guidance: Open video 07 and start watching from 12:30

## âš™ï¸ How to Update Videos
1. Add new videos to `video/`
2. Run:
   - step1_video_to_mp3.py
   - step2_mp3_to_json.py
   - step3_process_incoming.py
3. Restart Streamlit app


## ğŸ” Design Decisions
- Used cosine similarity for fast semantic matching
- Chunk-based retrieval for precise timestamps
- Local LLM via Ollama for privacy & cost efficiency
- Strict prompt to avoid hallucinations

## ğŸ”„ Pipeline

1. ğŸ¥ **Video Ingestion**  
   Input course videos are collected for processing.

2. ğŸ§ **Audio Extraction**  
   Videos are converted into audio files (MP4 â†’ MP3).

3. ğŸ“ **Speech-to-Text**  
   Audio is transcribed using Whisper with timestamps.

4. âœ‚ï¸ **Text Chunking**  
   Transcripts are split into meaningful chunks while preserving start and end times.

5. ğŸ§© **Embedding Generation**  
   Each text chunk is converted into semantic embeddings.

6. ğŸ“¦ **Vector Storage**  
   Embeddings along with metadata are stored for fast retrieval.

7. ğŸ” **Query Processing**  
   User query is embedded and compared using similarity search.

8. â±ï¸ **Timestamp Retrieval**  
   Most relevant video segment with exact timestamp is identified.

9. ğŸŒ **User Interface**  
   Streamlit app displays the answer and navigates to the correct video time.


## ğŸš€ Future Improvements

- Improve timestamp accuracy using finer text chunking  
- Enhance semantic search with better embedding models  
- Add keyword + semantic hybrid search  
- Optimize performance for faster query response  
- Support multiple languages  
- Develop a user-friendly web interface  
- Enable cloud-based storage and processing  
- Add user feedback to improve result relevance  
